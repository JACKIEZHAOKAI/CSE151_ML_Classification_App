{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T21:05:42.440325Z",
     "start_time": "2018-02-22T21:05:41.506414Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(datafile):\n",
    "    data = np.loadtxt(datafile,dtype={'names': ('str', 'label'),'formats': ('S10000', 'i')},delimiter=' ')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kernel function   time complexity: O(s+t)\n",
    "def kernel_function1(s, t, p):\n",
    "    count = 0\n",
    "    # create two dictionary to store all substrings of s and t with length p\n",
    "    # where key is the substring, value is the number of occurence \n",
    "    dict_s = {}\n",
    "    dict_t = {}\n",
    "    \n",
    "    #inserting num of appearance of all substrings of s and t into dictionary \n",
    "    for i in range(0, len(s) - p + 1):\n",
    "        s_substring = s[i:i+p]\n",
    "        if s_substring in dict_s:\n",
    "            dict_s[s_substring] += 1\n",
    "        else:\n",
    "             dict_s[s_substring] = 1\n",
    "    \n",
    "    for j in range(0, len(t) - p + 1):\n",
    "        t_substring = t[j:j+p]\n",
    "        if t_substring in dict_t:\n",
    "            dict_t[t_substring] += 1\n",
    "        else:\n",
    "             dict_t[t_substring] = 1\n",
    "            \n",
    "    #count the number of substrings of length p that are common to both s and t\n",
    "    count = 0\n",
    "    for key,val in dict_s.items():\n",
    "        if key in dict_t:\n",
    "            count += dict_s[key] * dict_t[key]\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-22T21:05:42.491013Z",
     "start_time": "2018-02-22T21:05:42.465003Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_dot_product(w_set, data_point, p):\n",
    "    total = 0\n",
    "    # dot product <wt, phi(xt)> is a linear combination of yi*Kp(xt,xi)\n",
    "    # the time complexity is O((s+t)*set size) = O((s+t)* n) in the worst case\n",
    "    for (protein_string, label) in w_set:\n",
    "        total += label * kernel_function1(protein_string, data_point, p)\n",
    "    return total\n",
    "\n",
    "# Implement the perceptron\n",
    "def kernelized_percetron(data, p):\n",
    "    # w_collection is a set of pairs that stores both data xt and its label yt\n",
    "    w_set = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        # compute yt * <wt, phi(xt)> and compare it with 0\n",
    "        if data[i][1] * kernel_dot_product(w_set, data[i][0], p) <= 0:\n",
    "            #Instead of computing wt+1, simply add the data point to the w set\n",
    "            w_set.append((data[i][0], data[i][1]))\n",
    "            \n",
    "    return w_set\n",
    "\n",
    "def kernelized_perceptron_prediction(w_collection, dataset, p):\n",
    "    errors = 0\n",
    "    total_num = dataset.shape[0]\n",
    "    for (protein_string, label) in dataset:\n",
    "        if label * kernel_dot_product(w_collection, protein_string, p) <= 0:\n",
    "            errors += 1\n",
    "    return float(errors)/ float(total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set (3630,)\n",
      "test_set (758,)\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "training_set = load_data('data/pa4train.txt')\n",
    "test_set = load_data('data/pa4test.txt')\n",
    "print \"training_set\",training_set.shape\n",
    "print \"test_set\", test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set (3630,)\n",
      "test_set (758,)\n",
      "P = 3:\n",
      "Training Error: 0.0134986225895\n",
      "Testing Error: 0.0422163588391\n",
      "time coast for p=3: 617.599425793\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print \"P = 3:\"\n",
    "w_set = kernelized_percetron(training_set, 3)\n",
    "print \"Training Error: \" + str(kernelized_perceptron_prediction(w_set, training_set, 3))\n",
    "print \"Testing Error: \" + str(kernelized_perceptron_prediction(w_set, test_set, 3))\n",
    "end = time.time()\n",
    "print \"time coast for p=3: \"+ str(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 4:\n",
      "Training Error: 0.00964187327824\n",
      "Testing Error: 0.0356200527704\n",
      "time coast for p=4: 503.892261028\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print \"P = 4:\"\n",
    "collections = kernelized_percetron(training_set, 4)\n",
    "print \"Training Error: \" + str(kernelized_perceptron_prediction(collections, training_set, 4))\n",
    "print \"Testing Error: \" + str(kernelized_perceptron_prediction(collections, test_set, 4))\n",
    "end = time.time()\n",
    "print \"time coast for p=4: \"+ str(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 5:\n",
      "Training Error: 0.0101928374656\n",
      "Testing Error: 0.0646437994723\n",
      "time coast for p=5: 787.495097876\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print \"P = 5:\"\n",
    "collections = kernelized_percetron(training_set, 5)\n",
    "print \"Training Error: \" + str(kernelized_perceptron_prediction(collections, training_set, 5))\n",
    "print \"Testing Error: \" + str(kernelized_perceptron_prediction(collections, test_set, 5))\n",
    "end = time.time()\n",
    "print \"time coast for p=5: \"+ str(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kernel function with slight modification s.t same sub-string only counted once\n",
    "#time complexity: O(s+t)\n",
    "def kernel_function2(s, t, p):\n",
    "    count = 0\n",
    "    # create two dictionary to store all substrings of s and t with length p\n",
    "    # where key is the substring, value is the number of occurence \n",
    "    dict_s = {}\n",
    "    dict_t = {}\n",
    "    \n",
    "    #inserting all substrings of s and t into dictionary \n",
    "    for i in range(0, len(s) - p + 1):\n",
    "        s_substring = s[i:i+p]\n",
    "        if s_substring not in dict_s:\n",
    "             dict_s[s_substring] = 1\n",
    "    \n",
    "    for j in range(0, len(t) - p + 1):\n",
    "        t_substring = t[j:j+p]\n",
    "        if t_substring not in dict_t:\n",
    "             dict_t[t_substring] = 1\n",
    "            \n",
    "    #count the number of substrings of length p that are common to both s and t\n",
    "    count = 0\n",
    "    for key,val in dict_s.items():\n",
    "        if key in dict_t:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_dot_product2(w_set, data_point, p):\n",
    "    total = 0\n",
    "    # dot product <wt, phi(xt)> is a linear combination of yi*Kp(xt,xi)\n",
    "    # the time complexity is O((s+t)*set size) = O((s+t)* n) in the worst case\n",
    "    for (protein_string, label) in w_set:\n",
    "        total += label * kernel_function2(protein_string, data_point, p)\n",
    "    return total\n",
    "\n",
    "# Implement the perceptron\n",
    "def kernelized_percetron2(data, p):\n",
    "    # w_collection is a set of pairs that stores both data xt and its label yt\n",
    "    w_set = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        # compute yt * <wt, phi(xt)> and compare it with 0\n",
    "        if data[i][1] * kernel_dot_product2(w_set, data[i][0], p) <= 0:\n",
    "            #Instead of computing wt+1, simply add the data point to the w set\n",
    "            w_set.append((data[i][0], data[i][1]))\n",
    "            \n",
    "    return w_set\n",
    "\n",
    "def kernelized_perceptron_prediction2(w_collection, dataset, p):\n",
    "    errors = 0\n",
    "    total_num = dataset.shape[0]\n",
    "    for (protein_string, label) in dataset:\n",
    "        if label * kernel_dot_product2(w_collection, protein_string, p) <= 0:\n",
    "            errors += 1\n",
    "    return float(errors)/ float(total_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set (3630,)\n",
      "test_set (758,)\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "training_set = load_data('data/pa4train.txt')\n",
    "test_set = load_data('data/pa4test.txt')\n",
    "print \"training_set\",training_set.shape\n",
    "print \"test_set\", test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 3:\n",
      "Training Error: 0.0132231404959\n",
      "Testing Error: 0.0554089709763\n",
      "time coast for p=3: 406.535048962\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "print \"P = 3:\"\n",
    "w_set = kernelized_percetron2(training_set, 3)\n",
    "print \"Training Error: \" + str(kernelized_perceptron_prediction2(w_set, training_set, 3))\n",
    "print \"Testing Error: \" + str(kernelized_perceptron_prediction2(w_set, test_set, 3))\n",
    "end = time.time()\n",
    "print \"time coast for p=3: \"+ str(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 4:\n",
      "Training Error: 0.00964187327824\n",
      "Testing Error: 0.0369393139842\n",
      "time coast for p=4: 440.401741982\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print \"P = 4:\"\n",
    "collections = kernelized_percetron2(training_set, 4)\n",
    "print \"Training Error: \" + str(kernelized_perceptron_prediction2(collections, training_set, 4))\n",
    "print \"Testing Error: \" + str(kernelized_perceptron_prediction2(collections, test_set, 4))\n",
    "end = time.time()\n",
    "print \"time coast for p=4: \"+ str(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 5:\n",
      "Training Error: 0.0101928374656\n",
      "Testing Error: 0.0646437994723\n",
      "time coast for p=5: 801.416409969\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print \"P = 5:\"\n",
    "collections = kernelized_percetron2(training_set, 5)\n",
    "print \"Training Error: \" + str(kernelized_perceptron_prediction2(collections, training_set, 5))\n",
    "print \"Testing Error: \" + str(kernelized_perceptron_prediction2(collections, test_set, 5))\n",
    "end = time.time()\n",
    "print \"time coast for p=5: \"+ str(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 5:\n",
      "Training Error: 0.0101928374656\n",
      "Testing Error: 0.0646437994723\n",
      "time coast for p=5: 1235.62603998\n"
     ]
    }
   ],
   "source": [
    "print \"P = 5:\"\n",
    "collections = kernelized_percetron(training_set, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countHighest(collections, p):\n",
    "    highest_substring_list = []\n",
    "    dictionary = {}\n",
    "    \n",
    "    for x, y in collections:\n",
    "        for i in range(0, len(x) - p + 1):\n",
    "            substring = x[i:i+p]\n",
    "            if substring in dictionary:\n",
    "                 dictionary[substring] += y*1\n",
    "            else:\n",
    "                 dictionary[substring] = 1\n",
    "                    \n",
    "    max_value = max(dictionary.iteritems(), key=operator.itemgetter(1))[1]\n",
    "    print \"the max value is: \", max_value\n",
    "    \n",
    "    for key,value in dictionary.items():\n",
    "        if value == max_value:\n",
    "            highest_substring_list.append(key)\n",
    "    \n",
    "    return highest_substring_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the substrings corresponding to these \n",
      "coordinates in w with the highest positive values\n",
      "the max value is:  3\n",
      "['KVGPD', 'WDTAG', 'LFLNK', 'DTAGQ', 'GKSSL']\n"
     ]
    }
   ],
   "source": [
    "print \"the substrings corresponding to these \\ncoordinates in w with the highest positive values\"\n",
    "print countHighest(collections,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "63px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
